---
title: "p8105_hw3_YJ2580"
author: "yj2580"
date: "10/7/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(p8105.datasets)
library(tidyverse)
library(ggplot2)
library(data.table)
library(gridExtra)
# load the data
data("instacart")
data("brfss_smart2010")
```

## Problem 1
This data has `r nrow(instacart)` entries and `r ncol(instacart)` variables in total. Each row of this table represents a detailed case of a specific purchasing.
Key variables are:
order_id: order identifier
product_id: product identifier
add_to_cart_order: order in which each product was added to cart
reordered: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
user_id: customer identifier
eval_set: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval_set)
order_number: the order sequence number for this user (1=first, n=nth)
order_dow: the day of the week on which the order was placed
order_hour_of_day: the hour of the day on which the order was placed
days_since_prior_order: days since the last order, capped at 30, NA if order_number=1
product_name: name of the product
aisle_id: aisle identifier
department_id: department identifier
aisle: the name of the aisle
department: the name of the department

For example, in the first row, the user 112108 made an order at 10 am on Thursday. The product is "Bulgarian Yogurt" from aisle "Yogurt", department "Dairy Eggs". The user had purchased this product before. It had been 9 days since the last order.

```{r}
# create a dataframe from instacart grouped by aisles and summarize the count of each aisle
aisle = instacart %>%
  group_by(aisle) %>%
  summarize(n = n())
# the total number of aisles
nrow(aisle)
# which aisles are the most items ordered from
aisle %>%
  arrange(desc(n)) %>%
  head(1) %>%
  pull(aisle)
```
Comment: there are 134 aisles, amongst which fresh vegetables are the most items ordered from.

```{r}
#choose aisles with more than 10000 items
aisle_new = aisle %>%
  filter(n>10000)
ggplot(aisle_new, aes(x = aisle, y = n)) + 
  geom_bar(stat = "identity")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "Aisle", y = "The number of items", title = " The number of items ordered in each aisle") 
```
Comments: There are `r nrow(aisle_new)` aisles with more than 10000 items ordered in total. As shown in the plot, items of fresh fruits and fresh vegetables which had been ordered obviously outnumbered those of other aisles, with around 150000 items. Items from majority of aisles had been ordered for below 25000 times.

```{r}
# find the three most popular items in aisles "baking ingredients"
bake_ingre = instacart %>%
  filter(aisle == "baking ingredients") %>%
  group_by(product_name) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head(3) %>%
  mutate(aisle = "baking ingredients")
# find the three most popular items in aisles “dog food care”
dog_food = instacart %>%
  filter(aisle == "dog food care") %>%
  group_by(product_name) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head(3) %>%
  mutate(aisle = "dog food care")
# find the three most popular items in aisles “packaged vegetables fruits”
pack_vege = instacart %>%
  filter(aisle == "packaged vegetables fruits") %>%
  group_by(product_name) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) %>%
  head(3) %>%
  mutate(aisle = "packaged vegetables fruits")
# combine three tables, the number of items is ordered
rbind(bake_ingre, dog_food, pack_vege) %>%
  arrange(n) %>%
  knitr::kable()
```

Comments: 
The three most popular items in aisles “dog food care” are Small Dog Biscuits, Organix Chicken & Brown Rice Recipe, and Snack Sticks Chicken & Rice Recipe Dog Treats, with the amount of 26, 28, and 30 respectively. This aisle sold least items compared with the other two.
The three most popular items in aisles “baking ingredients” are Cane Sugar, Pure Baking Soda, and Light Brown Sugar, with the amount of 336, 387, and 499 respectively.
The three most popular items in aisles “packaged vegetables fruits” are Organic Blueberries, Organic Raspberries, Organic Baby Spinach, with the amount of 4966, 5546, and 9784 respectively. This aisle sold most items compared with the other two.

```{r}
# show the mean hour of the day at which Pink Lady apples are ordered on each day of the week
pink_lady = instacart %>%
  filter(product_name %like% "Pink Lady Apples") %>%
  group_by(order_dow) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>%
  mutate(product_name = "Pink Lady Apples")
# show the mean hour of the day at which Coffee Ice Cream are ordered on each day of the week
coff_ice = instacart %>%
  filter(product_name %like% "Coffee Ice Cream") %>%
  group_by(order_dow) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>%
  mutate(product_name = "Coffee Ice Cream")
# combine two results and produce a 2 x 7 table
rbind(pink_lady, coff_ice) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_of_day
  ) %>%
  knitr::kable()
```

Comments: Pink Lady Apples were ordered from 11:00 to 14:00 approximately, while coffee ice cream were ordered from 12:00 to 15:30 approximately. 

## Problem 2
```{r}
# Create a dataframe
brfss = brfss_smart2010 %>%
  #clean variable names
  janitor::clean_names() %>%
  #focus on the “Overall Health” topic
  filter(topic == "Overall Health") %>%
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>%
  #order from "poor" to "excellent"
  arrange(response)
```

```{r}
# In 2002, which states were observed at 7 or more locations
brfss %>%
  filter(year == "2002") %>%
  group_by(locationdesc) %>%
  summarize(n = n()) %>%
  mutate(locationabbr = substr(locationdesc, 1, 2)) %>%
  group_by(locationabbr) %>%
  summarize(n = n()) %>%
  filter(n >= 7) %>%
  pull(locationabbr)
# In 2010, which states were observed at 7 or more locations
brfss %>%
  filter(year == "2010") %>%
  group_by(locationdesc) %>%
  summarize(n = n()) %>%
  mutate(locationabbr = substr(locationdesc, 1, 2)) %>%
  group_by(locationabbr) %>%
  summarize(n = n()) %>%
  filter(n >= 7) %>%
  pull(locationabbr)
```
Comments:
In 2002, there are 6 states ("CT" "FL" "MA" "NC" "NJ" "PA") were observed at 7 or more locations.
In 2010, there are 14 states ("CA" "CO" "FL" "MA" "MD" "NC" "NE" "NJ" "NY" "OH" "PA" "SC" "TX" "WA") were observed at 7 or more locations.

```{r}
# Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. 
mean_state_value = brfss %>%
  filter(response == "Excellent") %>%
  group_by(locationabbr, year) %>%
  summarize(mean_state_value = mean(data_value, na.rm = TRUE))
names(mean_state_value)[1] = "state"
mean_state_value %>%
  group_by(state) %>%
  summarise(n = n())
# Make a “spaghetti” plot of this average value over time within a state
ggplot(mean_state_value, aes(x=year, y=mean_state_value, color=state)) + geom_line() + labs(x = "Year", y = "Average value", title = "Average value over time within a state")
```
Comments: Average value over time in the same state doesn't fluctuate a lot over time. However, there are some difference of the range of average value between different states over time. Among these, the average value in WV is obviously smaller than that in other states. Overall, this plot is not intuitive enough to see the pattern clearly because there are too many states and some colors look very similar. We could better divide these states into groups(eg. "East" and "West"), then make a two panel plot to help analysis. 

```{r}
# Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State
year06 = brfss %>%
  filter(year == "2006",
         locationabbr == "NY") %>%
  group_by(locationdesc)
year10 = brfss %>%
  filter(year == "2010",
         locationabbr == "NY") %>%
  group_by(locationdesc)

p1 = ggplot(year06, aes(x = response, y = data_value, color = locationdesc, group = locationdesc)) + geom_point() + geom_line() + labs(title = "In 2006, distribution of data value for responses in NY", x = "Responce", y = "Data value")
p2 = ggplot(year10, aes(x = response, y = data_value, color = locationdesc, group = locationdesc)) + geom_point() + geom_line() + labs(title = "In 2010, distribution of data value for responses in NY", x = "Responce", y = "Data value")

grid.arrange(p1, p2, nrow = 2)
```
Comments: From 2006 to 2010, the system enlarged its data collecting range from six counties to nine counties. The patterns of responce vs data value almost stay the same from 2006 to 2010. Data value increases from poor responce to good responce, then its peak appears in either good responce or very good responce, finally the value decreases when it goes to excellent responce.    

## Problem 3

```{r}
#load activity data
accel = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  #create a weekday vs weekend variable
  mutate(weekend_weekday = ifelse(day == "Sunday" | day == "Saturday", "weekend", "weekday")) %>%
  pivot_longer(
    activity_1 : activity_1440,
    names_to = "minute", 
    names_prefix = "activity_",
    values_to = "activity_counts"
  ) %>%
  mutate(minute = factor(minute, levels = c(1:1440)))
```
Comments:
There are `r ncol(accel)` variables and `r nrow(accel)` observations in total after tidying the data.
Variables: 
week: in which week the activity counts were recorded
day_id: on which day the activity counts were recorded
day: the day of the week
weekend_weekday: whether the day is weekdays or weekends
minute: at which minute of that day the activity count was recorded
activity_counts: activity counts record

```{r}
accel %>%
  group_by(day_id, day, week) %>%
  summarize(total_counts = sum(activity_counts)) %>%
  knitr::kable()
```

Comments: On day2, day24 and day31, the activity count was obviously lower. On Saturdays of week 4 and 5 (day24 and day31), there is only one activity count for each minute of 24 hours. The data for these two days is abnormal, so we could explore the reason behind. Maybe the accelerometer was worn unobstrusively. In that case, we should probably omit these data when analyzing or evaluating the trend. On day 4, day 10, day 16, day 29, the activity counts are relatively high, with the amount of over 600000. However, there is no apparent trend from this table.

```{r}
accel %>%
  ggplot(aes(x = minute, y = activity_counts, color = day)) + geom_line(alpha = .5) + scale_x_discrete(breaks=seq(60,1440,60), labels = as.character(c(1:24))) + labs(x="activity hours", y="activity counts", title="24 hour activity time courses for each day")
```
Comments: 
As shown in the plot, most activity counts for each minute are below 2500. From 19:00 to 22:00, there is a extremely high frequency of high activity counts, mostly up to 5000. The highest activity count is recorded on Wednesday during this period. From 16:00 to 17:00, there are also some high activity counts recorded, especially on Saturday. From 11:00 to 12:00 on Sunday, the activity counts are relatively high compared with other days. On Thursday, the activity counts are highest around 7:00. 

